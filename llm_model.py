from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

class LLMModel:
    def __init__(self, model_name="Qwen/Qwen-7B-Chat"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰GPUå¯ç”¨
        device_map = "auto" if torch.cuda.is_available() else {"": "cpu"}
        
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            device_map=device_map,
            trust_remote_code=True
        )
        self.model.eval()

    def generate_description(self, predictions):
        # æ„å»º promptï¼ˆå­—ç¬¦ä¸²æ ¼å¼ï¼‰
        prompt = "ä½ æ˜¯ä¸€ä¸ªå›¾åƒæè¿°åŠ©æ‰‹ã€‚\n"
        prompt += "è¯·æ ¹æ®ä»¥ä¸‹æ£€æµ‹ç»“æœç”Ÿæˆä¸€æ®µè‡ªç„¶è¯­è¨€æè¿°ï¼š\n"
        for pred in predictions:
            prompt += f"- {pred['label']} (ç½®ä¿¡åº¦: {pred['confidence']:.2f})\n"
        prompt += "è¯·ç”¨ç®€æ´ã€ç”ŸåŠ¨çš„è¯­è¨€æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ã€‚\n"

        # ğŸ”‘ ä½¿ç”¨ Qwen è‡ªå¸¦çš„ chat æ–¹æ³•ï¼ˆä¼ å…¥å­—ç¬¦ä¸²ï¼Œä¸æ˜¯ messages åˆ—è¡¨ï¼‰
        response, _ = self.model.chat(self.tokenizer, prompt, history=None)
        return response
